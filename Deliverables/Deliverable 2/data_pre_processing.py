# -*- coding: utf-8 -*-
"""Data Pre-processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NCb0TvbgjGa6xsbRVFgwsFJ63QnqYcJo
"""

#Author: Eyas Hassan
#Processing strategy inspired by Valerio Velardo

import json
import os
import math
import librosa
import numpy as np
import pandas as pd


NON_GUITAR_DATA_PATH = "/content/drive/MyDrive/AutoChordRecognition Dataset/non_guitar"

NON_GUITAR_JSON_PATH = os.path.join(NON_GUITAR_DATA_PATH, "non-guitar-2.json")
NON_GUITAR_LABEL_PATH = os.path.join(NON_GUITAR_DATA_PATH, "non_guitar_annotation.txt")

SAMPLE_RATE = 22050 # measured in Hz
TRACK_DURATIONS = 1152 # measured in s
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATIONS

def save_mel_spectrogram(dataset_path, json_path, num_segments, n_fft=2048, hop_length=512):
  """Extracts MELs from music dataset and saves them into a json file along witgh genre labels.
      :param dataset_path (str): Path to dataset
      :param json_path (str): Path to json file used to save MELs
      :param: num_segments (int): Number of segments we want to divide sample tracks into
      :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples
      :param hop_length (int): Sliding window for FFT. Measured in # of samples
      :return:
      """

  # dictionary to store mapping, labels, and MELs
  data = {"mapping": [],
          "labels": [],
          "MEL": []
          }

  samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
  num_mel_vectors_per_segment = math.ceil(samples_per_segment / hop_length)

  # generating label list
  label_list = pd.read_csv(NON_GUITAR_LABEL_PATH, delimiter= '\s+', index_col=False, header=None)
  label_list = label_list[2].tolist()

  # generating corresponding mapping
  voicing = {}
  counter = 0
  
  for chord in label_list:
    if chord not in voicing:
      voicing[chord] = counter
      counter += 1

  data["mapping"].append([*voicing])

  # loop through instrument samples
  for instrument in os.listdir(dataset_path):
    
    # handling the audio files
    if instrument.endswith(".wav"):
      sample_path = os.path.join(dataset_path, instrument)
      signal, sample_rate = librosa.load(sample_path, sr=SAMPLE_RATE)

      # segmenting sample into its constituent 2 second chord voicing
      for s in range(num_segments):
        
        # calculating start and end sample for each chord voicing
        start = s * samples_per_segment
        end = start + samples_per_segment

        # extract log spaced frequency, log amplitude mel spectogram
        segment = signal[start:end]
        spectrogram = librosa.feature.melspectrogram(segment, hop_length=hop_length, n_fft=2048, sr=sr, n_mels=133, window="hann")
        mel_spectrogram = librosa.power_to_db(spectrogram)
        mel_spectrogram = mel_spectrogram.T
        
        # store only spectrogram with expected number of vectors & append corresponding label
        if len(mel_spectrogram) == num_mel_vectors_per_segment:
            data["MEL"].append(mel_spectrogram.tolist())
            print("{}, chord:{}".format(sample_path, s+1))
            data["labels"].append(voicing[label_list[s]])
  
  with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)

# 576 is the number of 2 second chord segments present in a wav file
save_mel_spectrogram(NON_GUITAR_DATA_PATH, NON_GUITAR_JSON_PATH, 576)